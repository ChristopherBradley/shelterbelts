# +
# Train a neural network to compare with random forest predictions

# +
# # !pip install pyarrow # For loading .feather files

# +
# # Change directory to this repo - this should work on gadi or locally via python or jupyter. Need this when using the DEA environment.
import os, sys
repo_name = "shelterbelts"
if os.path.expanduser("~").startswith("/home/"):  # Running on Gadi
    repo_dir = os.path.join(os.path.expanduser("~"), f"Projects/{repo_name}")
elif os.path.basename(os.getcwd()) != repo_name:  # Running in a jupyter notebook 
    repo_dir = os.path.dirname(os.getcwd())       
else:                                             # Already running from root of this repo. 
    repo_dir = os.getcwd()
src_dir = os.path.join(repo_dir, 'src')
os.chdir(src_dir)
sys.path.append(src_dir)
# print(src_dir)


# +
import os
import pickle

import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# -

from shelterbelts.classifications.neural_network import my_train_test_split, inputs_outputs_split, class_accuracies_overall # Might need to adjust this to work with the random forst model


def random_forest(training_file, outdir=".", stub="TEST", output_column='tree_cover', drop_columns=['x', 'y', 'tile_id'], n_estimators=100, min_samples_split=10, batch_size=32, random_state=1, stratification_columns=['tree_cover'], train_frac = 0.7, limit=None):
    """
    Create and evaluate a random forest to predict tree vs no tree classifications

    Parameters
    ----------
        training_file: Either a .feather or .csv file, generated by merge_inputs_outputs.csv
        outdir: The directory of the training feather file
        stub: The suffix of the training feather file 
        output_column: Column with the output data
        drop_columns: The columns that aren't inputs or outputs (just extra metadata about that sample)
        n_estimators: hyper_parameter for tuning
        min_samples_split: hyper_parameter for tuning
        stratification_columns: Whether to normalise the number of samples from each class (reduces the overall number of training samples)
        train_frac: Percentage of training vs testing samples. Additionally, should reserve some extra tiles for validation.
        limit: Number of rows to use when training the model

    Returns
    -------
        df_accuracy: precision, recall, specificity, sensitivity, accuracy for 0's and 1's - and grouped by category

    Downloads
    ---------
        accuracy_metrics: The df_accuracy as a csv
        model.keras: The machine learning model for running on new input data
        scaler.pkl: The standard scaler used to normalise the input data
        training.png: accuracy and loss plots over each epoch
    """
    if training_file.endswith('.feather'):
        df = pd.read_feather(training_file)
    else:
        df = pd.read_csv(training_file)
        
    df_train, df_test = my_train_test_split(df, stratification_columns, train_frac, random_state)

    non_input_columns = [output_column] + drop_columns # ['koppen_class']  
    X_train, X_test, y_train, y_test, scaler = inputs_outputs_split(df_train, df_test, outdir, stub, non_input_columns, output_column)

    model = RandomForestClassifier(n_estimators=n_estimators, min_samples_split=min_samples_split, random_state=random_state)
    model.fit(X_train, y_train)

    filename = os.path.join(outdir, f'{stub}_random_forest.pkl')
    with open(filename, "wb") as f:
        pickle.dump(model, f)
    print("Saved:", filename)

    df_accuracy = class_accuracies_overall(df_test, model, scaler, outdir, stub, non_input_columns, output_column)
    return df_accuracy


# +
import argparse

def parse_arguments():
    """Parse command line arguments with default values."""
    parser = argparse.ArgumentParser()
    
    parser.add_argument('training_file', help='Either a .feather or .csv file, generated by merge_inputs_outputs.csv')
    parser.add_argument('--outdir', default='.', help='Directory of the outputs')
    parser.add_argument('--stub', default='TEST_RF', help='Prefix of the outputs')
    parser.add_argument('--output_column', default='tree_cover', help='Column with the output data (default: tree_cover)')
    parser.add_argument('--drop_columns', nargs='+', default=['x', 'y', 'tile_id'], help='Columns to drop (default: x y tile_id)')
    parser.add_argument('--random_state', type=int, default=1, help='Random seed (default: 1)')
    parser.add_argument('--stratification_columns', nargs='+', default=['tree_cover'], help='Columns to stratify samples on (default: tree_cover)')
    parser.add_argument('--train_frac', type=float, default=0.7, help='Fraction of samples to use for training (default: 0.7)')
    parser.add_argument('--limit', type=int, default=None, help='Number of rows to use when training (default: all)')

    return parser.parse_args()


if __name__ == '__main__':
    args = parse_arguments()
    
    random_forest(
        args.training_file,
        outdir=args.outdir,
        stub=args.stub,
        output_column=args.output_column,
        drop_columns=args.drop_columns,
        random_state=args.random_state,
        stratification_columns=args.stratification_columns,
        train_frac=args.train_frac,
        limit=args.limit
    )


# +
# # # %%time
# training_file = '/scratch/xe2/cb8590/alpha_earth_embeddings.csv'
# df = random_forest(training_file, outdir="/scratch/xe2/cb8590/tmp/", stub="alpha_earth", output_column='tree', drop_columns=[], stratification_columns=['tree'])
# df = pd.read_csv('/scratch/xe2/cb8590/tmp/alpha_earth_metrics.csv')
# df
# # Precision 93%, recall 94%, accuracy 94%
